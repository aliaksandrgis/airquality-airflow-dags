version: "3.8"

services:
  airflow-postgres:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data

  airflow-webserver:
    build:
      context: .
      dockerfile: airflow.Dockerfile
    image: airquality-airflow:2.8.1
    user: "0:0"
    restart: unless-stopped
    depends_on:
      - airflow-postgres
    env_file:
      - ./.env.airflow
      - ./.env.pipeline
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    volumes:
      - ../../dags:/opt/airflow/dags
      - ./volumes/airflow/logs:/opt/airflow/logs
      - ./volumes/airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "127.0.0.1:8080:8080"
    command: webserver

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow.Dockerfile
    image: airquality-airflow:2.8.1
    user: "0:0"
    restart: unless-stopped
    depends_on:
      - airflow-postgres
    env_file:
      - ./.env.airflow
      - ./.env.pipeline
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    volumes:
      - ../../dags:/opt/airflow/dags
      - ./volumes/airflow/logs:/opt/airflow/logs
      - ./volumes/airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler

  spark-live:
    build:
      context: ../../../airquality-spark-jobs
    restart: unless-stopped
    env_file:
      - ./.env.spark
    environment:
      TMPDIR: /data/tmp
      SPARK_LOCAL_DIRS: /data/tmp
    volumes:
      - ./data/bronze:/data/bronze
      - ./data/tmp:/data/tmp
    command:
      - --master
      - local[2]
      - --driver-memory
      - 2g
      - --conf
      - spark.ui.enabled=false
      - --conf
      - spark.sql.shuffle.partitions=8
      - jobs/live_measurements.py

  producer:
    build:
      context: ../../../airquality-data-pipeline
    restart: "no"
    env_file:
      - ./.env.pipeline
    command: ["app.main"]

volumes:
  airflow_postgres_data:
